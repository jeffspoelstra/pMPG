summary(modelFitall)
confusionMatrix(training$diagnosis, predict(modelFitall,pvars))
modelFitall<-train(training$diagnosis ~ ., method="lm", data=pvars)
summary(modelFitall)
confusionMatrix(training$diagnosis, predict(modelFitall,pvars))
pvnames <- c('diagnosis', 'IL_11','IL_13','IL_16','IL_17E','IL_1alpha','IL_3','IL_4','IL_5',
'IL_6','IL_6_Receptor','IL_7','IL_8')
pvars<-training[,pvnames]
modelFitall<-train(diagnosis ~ ., method="lm", data=pvars)
summary(modelFitall)
confusionMatrix(training$diagnosis, predict(modelFitall,pvars))
modelFitall<-lm(diagnosis ~ ., data=pvars)
modelFitall<-train(diagnosis ~ ., method="lm", data=pvars)
modelFitall<-train(diagnosis ~ ., method="lm", data=pvars)
modelFitall<-train(diagnosis ~ ., data=pvars, method="lm")
names(getModelInfo())
qnorm(0.9)
qnorm(0.95)
pnorm(0.95)
bnorm(0.95)
dnorm(0.95)
qnorm(1)
qnorm(0.97)
qnorm(1-0.025)
qnorm(1-0.05)
install.packages("shiny")
library(statsr)
library(dplyr)
library(ggplot2)
data(ames)
ames %>%
summarise(mu = mean(area), pop_med = median(area),
sigma = sd(area), pop_iqr = IQR(area),
pop_min = min(area), pop_max = max(area),
pop_q1 = quantile(area, 0.25),  # first quartile, 25th percentile
pop_q3 = quantile(area, 0.75))  # third quartile, 75th percentile
samp1 <- ames %>%
sample_n(size = 50)
samp1 %>%
summarise(xbar = mean(area), pop_med = median(area),
sd = sd(area), pop_iqr = IQR(area),
pop_min = min(area), pop_max = max(area),
pop_q1 = quantile(area, 0.25),  # first quartile, 25th percentile
pop_q3 = quantile(area, 0.75))  # third quartile, 75th percentile
ggplot(data = samp1, aes(x = area)) +
geom_histogram(binwidth = 250)
sample_means_small <- ames %>%
rep_sample_n(size = 10, reps = 25, replace = TRUE) %>%
summarise(x_bar = mean(area))
sample_means50 <- ames %>%
rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means50, aes(x = x_bar)) +
geom_histogram(binwidth = 20)
sample_means50 <- ames %>%
rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means50, aes(x = x_bar)) +
geom_histogram(binwidth = 10000)
sample_means50 <- ames %>%
rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means50, aes(x = x_bar)) +
geom_histogram(binwidth = 5000)
sample_means50 <- ames %>%
rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means50, aes(x = x_bar)) +
geom_histogram(binwidth = 1000)
sample_means50 <- ames %>%
rep_sample_n(size = 50, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means50, aes(x = x_bar)) +
geom_histogram(binwidth = 5000)
sample_means150 <- ames %>%
rep_sample_n(size = 150, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means150, aes(x = x_bar)) +
geom_histogram(binwidth = 5000)
sample_means150 <- ames %>%
rep_sample_n(size = 150, reps = 5000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means150, aes(x = x_bar)) +
geom_histogram(binwidth = 2500)
ames %>%
sample_n(size = 15) %>%
summarise(x_bar = mean(price))
sample_means15 <- ames %>%
rep_sample_n(size = 15, reps = 2000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means15, aes(x = x_bar)) +
geom_histogram(binwidth = 2500)
sample_means150 <- ames %>%
rep_sample_n(size = 150, reps = 2000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means15, aes(x = x_bar)) +
geom_histogram(binwidth = 2500)
sample_means150 <- ames %>%
rep_sample_n(size = 150, reps = 2000, replace = TRUE) %>%
summarise(x_bar = mean(price))
ggplot(data = sample_means15, aes(x = x_bar)) +
geom_histogram(binwidth = 5000)
install.packages("AppliedPredictiveModeling","ElemStatLearn","pgmm","rpart")
install.packages("pgmm","rpart")
install.packages("ElemStatLearn")
install.packages("AppliedPredictiveModeling")
install.packages("pgmm")
install.packages("rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain<-createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
training<-segmentationOriginal[inTrain,]
testing<-segmentationOriginal[-inTrain,]
set.seed(125)
modFit<-train(Case ~ .,method='rpart',data=training)
predict(modFit,newdata=as.data.frame(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2))
print(modFit$finalModel)
modFit<-train(Case ~ .,method='rpart',data=training)
print(modFit$finalModel)
set.seed(125)
inTrain<-createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
training<-segmentationOriginal[inTrain,]
testing<-segmentationOriginal[-inTrain,]
modFit<-train(Case ~ .,method='rpart',data=training)
print(modFit$finalModel)
inTrain<-createDataPartition(y=segmentationOriginal$Class,p=0.7,list=FALSE)
training<-segmentationOriginal[inTrain,]
testing<-segmentationOriginal[-inTrain,]
modFit<-train(Case ~ .,method='rpart',data=training)
print(modFit$finalModel)
modFit<-train(Class ~ .,method='rpart',data=training)
print(modFit$finalModel)
install.packages('rattle')
library(rattle)
library(rattle)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rattle)
inTrain<-createDataPartition(y=segmentationOriginal$Class,p=0.7,list=FALSE)
training<-segmentationOriginal[inTrain,]
testing<-segmentationOriginal[-inTrain,]
modFit<-train(Class ~ .,method='rpart',data=training)
fancyRpartPlot(modFit$finalModel)
print(modFit$finalModel)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rattle)
#inTrain<-createDataPartition(y=segmentationOriginal$Class,p=0.7,list=FALSE)
training<-segmentationOriginal[segmentationOriginal$Class=='Train',]
testing<-segmentationOriginal[segmentationOriginal$Class=='Test',]
modFit<-train(Class ~ .,method='rpart',data=training)
fancyRpartPlot(modFit$finalModel)
training<-segmentationOriginal[segmentationOriginal$Case=='Train',]
testing<-segmentationOriginal[segmentationOriginal$Case=='Test',]
modFit<-train(Class ~ .,method='rpart',data=training)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive=olive[,-1]
newdata=as.data.frame(t(colMeans(olive)))
modFit<-train(Area ~ .,method='rpart',data=olive)
library(caret)
modFit<-train(Area ~ .,method='rpart',data=olive)
predict()
predict(modFit,newdata=newdata)
summary(olive$Area)
library(rattle)
fancyRpartPlot(modFit$finalModel)
rm(list=ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[â€train,]
set.seed(13234)
modFit<-train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,method='glm',family='binomial',data=training)
modFit<-train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,method='glm',family='binomial',data=trainSA)
dim(SAheart[1])
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
pvals <- predict(modFit,newdata=trainSA)
missClass(trainSA$chd,predict(modFit,newdata=trainSA))
missClass(trainSA$chd,predict(modFit,newdata=testSA))
missClass(testSA$chd,predict(modFit,newdata=testSA))
rm(list=ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
dim(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modFit<-train(y~.,model='rf',data=vowel.train,prox=TRUE)
varImp(modFit)
varImp(modFit, scale=FALSE)
modFit<-train(y~.,model='rf',data=vowel.train)
varImp(modFit, scale=FALSE)
modFit<-train(y~.,model='rf',data=vowel.test)
varImp(modFit, scale=FALSE)
pnorm(2.154)
pnorm(-2.154)
pnorm(-2.154)*2
qnorm(0.05)
pnorm(2.582, lower.tail = FALSE)
pnorm(-2.874)
pnorm(-1.392)
pnorm(1.392, lower.tail = FALSE)
set.seed(9102015)                 # make sure to change the seed
library(statsr)
library(dplyr)
library(ggplot2)
data(ames)
n <- 60
samp <- sample_n(ames, n)
# type your code for the Exercise here, and Knit
hist(samp$area)
params <- ames %>%
summarise(mu = mean(area))
samp %>%
summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)))
z_star_95 <- qnorm(0.975)
z_star_95
samp %>%
summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)))
ci <- ames %>%
rep_sample_n(size = n, reps = 50, replace = TRUE) %>%
summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)))
ci %>%
slice(1:5)
ci <- ci %>%
mutate(capture_mu = ifelse(lower < params$mu & upper > params$mu, "yes", "no"))
ci_data <- data.frame(ci_id = c(1:50, 1:50),
ci_bounds = c(ci$lower, ci$upper),
capture_mu = c(ci$capture, ci$capture))
ggplot(data = ci_data, aes(x = ci_bounds, y = ci_id,
group = ci_id, color = capture_mu)) +
geom_point(size = 2) +  # add points at the ends, size = 2
geom_line() +           # connect with lines
geom_vline(xintercept = params$mu, color = "darkgray") # draw vertical line
qnorm(0.995)
z_star_95 <- qnorm(0.995)
z_star_95
samp %>%
summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)))
params <- ames %>%
summarise(mu = mean(area))
ci <- ames %>%
rep_sample_n(size = n, reps = 50, replace = TRUE) %>%
summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)))
ci <- ci %>%
mutate(capture_mu = ifelse(lower < params$mu & upper > params$mu, "yes", "no"))
ci_data <- data.frame(ci_id = c(1:50, 1:50),
ci_bounds = c(ci$lower, ci$upper),
capture_mu = c(ci$capture, ci$capture))
ggplot(data = ci_data, aes(x = ci_bounds, y = ci_id,
group = ci_id, color = capture_mu)) +
geom_point(size = 2) +  # add points at the ends, size = 2
geom_line() +           # connect with lines
geom_vline(xintercept = params$mu, color = "darkgray") # draw vertical line
rm(list=ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modFit<-train(y~.,model='rf',data=vowel.train)
library(caret)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modFit<-train(y~.,model='rf',data=vowel.train)
modFitgbm<-train(y~.,model='gbm',data=vowel.train)
predrf <- predict(modFit, vowel.test)
confusionMatrix(predrf$class, vowel.test$y)
confusionMatrix(predrf, vowel.test$y)
predgbm <- predict(modFitgbm, vowel.test)
confusionMatrix(predgbm, vowel.test$y)
modagree<-predgbm==predrf
sum(modagree)
confusionMatrix(predgbm, vowel.test[modagree,'y'])
vt<-vowel.test[modagree,]
pgbm<-predgbm[modagree,]
pgbm<-predgbm[modagree]
vt<-vowel.test[modagree,'y']
confusionMatrix(pgbm, vt)
rm(list=ls())
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFitrf<-train(diagnosis~.,model='rf',data=training)
modFitgbm<-train(diagnosis~.,model='gbm',data=training)
modFitlda<-train(diagnosis~.,model='lda',data=training)
predrf <- predict(modFitrf, testing)
confusionMatrix(predrf, testing$diagnosis)
predgbm <- predict(modFitgbm, testing)
confusionMatrix(predgbm, testing$diagnosis)
predlda <- predict(modFitlda, testing)
confusionMatrix(predlda, testing$diagnosis)
predDF<-data.frame(predrf,predgbm,predlda,diagnosis=testing$diagnosis)
combModFit<-train(diagnosis~.,method='rf',data=predDF)
combPred<-predict(combModFit,predDF)
confusionMatrix(combPred,predDF$diagnosis)
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
modFit<-train(CompressiveStrength~.,method='lasso',data=training)
?plot.enet
plot(modFit, "lambda", label=TRUE)
plot(modFit)
plot(modFit, xvar='penalty')
plot(modFit, xvar='fraction')
plot(modFit, xvar='L1norm')
plot(modFit, xvar='step')
plot.enet(lassoFit$finalModel, xvar="penalty", use.color=T)
plot.enet(modFit$finalModel, xvar="penalty", use.color=T)
library(lubridate)
library(forecast)
dat = read.csv("./gaData.csv")
training = dat[year(dat$date)==2011,]
tstrain = ts(training$visitsTumblr)
testing = dat[year(dat$date)>2011,]
test = dat[year(dat$date) > 2011,]
pred <- forecast(fit, h=length(test$visitsTumblr),level=c(80,95))
fcast <- forecast(fit)
plot(fcast)
accuracy(fcast,test$visitsTumblr)
modBats <- bats(tstrain)
pred <- forecast(modBats, h=length(testing$visitsTumblr),level=c(80,95))
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
accuracy <- 1-sum(test$visitsTumblr>pred$upper[,2])/length(test$visitsTumblr)
install.packages('forecast')
library(forecast)
dat = read.csv("./gaData.csv")
dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("~/../Desktop/gaData.csv")
rm(list=ls())
dat = read.csv("~/../Desktop/gaData.csv")
training = dat[year(dat$date)==2011,]
tstrain = ts(training$visitsTumblr)
testing = dat[year(dat$date)>2011,]
test = dat[year(dat$date) > 2011,]
pred <- forecast(fit, h=length(test$visitsTumblr),level=c(80,95))
modBats <- bats(tstrain)
pred <- forecast(modBats, h=length(testing$visitsTumblr),level=c(80,95))
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
accuracy <- 1-sum(test$visitsTumblr>pred$upper[,2])/length(test$visitsTumblr)
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
accuracy <- 1-sum(test$visitsTumblr>pred$upper[,2])/length(test$visitsTumblr)
library(e1071)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svmFit <- svm(CompressiveStrength ~ ., data = training)
svmPred <- predict(svmFit,testing)
accuracy(svmPred, testing$CompressiveStrength)
install.packages(c("BH", "DBI", "fields", "formatR", "git2r", "highr", "jsonlite", "knitr", "nlme", "openssl", "quantreg", "Rcpp", "RcppArmadillo", "RMySQL", "stringi", "survival"))
install.packages("installr")
library(installr)
updateR()
pt(-2.24,21)*2
pt(-0.545,199)*2
qt(-0.545,199)*2
pt(-0.545,199)
dt(-0.545,199)
pt(-0.87,199)
pnorm(-0.2)
qnorm(.8)
qnorm(.95)
qnorm(.975)
qt(.9,5)
qt(.98,20)
qt(.95,28)
qt(.99,11)
qt(.9,5, lower.tail = FALSE)
pt(.9,5, lower.tail = FALSE)
pt(.8,5)
pt(.95,5, lower.tail = FALSE)
pt(.95,5)
qt(.95,5)
qt(.99,20)
qt(.975,28)
qt(.995,11)
pt(1.91,10)
pt(1.91,10, lower.tail = FALSE)
pt(-3.45,16)
pt(0.83,6)
pt(0.83,6, lower.tail = FALSE)
pt(0.83,6, lower.tail = FALSE)*2
pt(2.13,27, lower.tail = FALSE)
(21.015-18.985)/2
qt(.975,35)
pt(-.351,24)
pt(-1.753,24)
qt(.975,19)
qt(.05,19)
qt(.975,19)
pt(-.813,19)
pnorm(1.603)
pnorm(1.603, lower.tail = false)
pnorm(1.603, lower.tail = F)
pt(1.603,50,lower.tail = F)
qnorm(.05)*2
qnorm(.05)
pnorm(2.72,lower.tail = F) * 2
pt(4.93,9,lower.tail = F)
pt(-2.707,5)*2
qt(.975,5,lower.tail = F)
qt(.975,5,lower.tail = FALSE)
qt(.975,5)
qt(.975,9)
pt(3.017,9,lower.tail = F)*2
pt(3.274,11,lower.tail = F)*2
qnorm(.9,lower.tail = F)
library("devtools", lib.loc="C:/Program Files/R/R-3.3.0/library")
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
qnorm(.99)
qnorm(.90)
qnorm(.95)
qnorm(.995)
qnorm(.975)
pnorm(1.65,lower.tail = FALSE)
pnorm(1.65,lower.tail = FALSE)*2
pnorm(2.913,lower.tail = FALSE)*2
pchisq(7.515,2)
pchisq(7.515,2,lower.tail = F)
pchisq(11.58,2,lower.tail = F)
library(statsr)
library(dplyr)
library(ggplot2)
data(atheism)
head(atheism)
summary(atheism)
View(atheism)
View(atheism)
us12 <- atheism %>%
filter(nationality == "United States" , atheism$year == "2012")
sum(us12$response=='atheist')/1002
sp <- atheism %>%
filter(nationality == "Spain")
sp <- atheism %>%
filter(nationality == "Spain") %>% group_by(year)
inference(y = response, data = sp, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
head(sp)
summary(sp)
inference(y = response, x = year, data = sp, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
us <- atheism %>%
filter(nationality == "United States")
inference(y = response, x = year, data = us, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
library(shiny)
setwd("C:/Users/jeff/Desktop/JHU-DA-class/data-products")
runApp()
luibrary(datasets)
library(datasets)
data(mtcars)
View(mtcars)
runApp()
min(mtcars$wt)
max(mtcars$wt)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
hist(mtcars$cyl, main='Predicted MPG', xlab='Cylinders', col='lightblue')
runApp()
runApp()
